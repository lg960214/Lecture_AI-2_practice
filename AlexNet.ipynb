{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. System settings\n",
    "2. Dataset & Dataloader & Augmentation\n",
    "3. Model\n",
    "4. Optimizer & Loss func\n",
    "5. Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): 정답 label의 classification 갯수\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__() # nn.module을 상속받음\n",
    "        # input size : (b x 3 x 227 x 227)\n",
    "        \n",
    "        self.net = nn.Sequential( # nn.Sequential : model의 층을 연속적으로 쌓아주는 함수\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3에 나와 있는 hyperparameter 값 사용  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
    "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
    "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=False), # inplace=True : 계산한 값을 변수에 덮어씌움 / True 시 오류가 나서 False로 통일\n",
    "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\n",
    "        )\n",
    "        self.init_bias()  # 아래 함수 참고\n",
    "\n",
    "    def init_bias(self):\n",
    "        for layer in self.net: # self.net에 정의된 layer에서\n",
    "            if isinstance(layer, nn.Conv2d): # 해당 layer가 Conv2D layer 라면 \n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01) # Normal distribution으로 weight initialization 진행\n",
    "                nn.init.constant_(layer.bias, 0) # Bias를 모두 0으로 initialization\n",
    "\n",
    "        nn.init.constant_(self.net[4].bias, 1) # 2번째 Conv2D layer bias를 1로 초기화\n",
    "        nn.init.constant_(self.net[10].bias, 1) # 4번째 Conv2D layer의 bias를 1로 초기화\n",
    "        nn.init.constant_(self.net[12].bias, 1) # 5번째 Conv2D layer의 bias를 1로 초기화\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(-1, 256 * 6 * 6)  # view는 tensor shape를 바꿔줌, FC layer로 넘겨주기 위해 output을 1차원으로 펴주는 역할\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([128, 3, 227, 227])\n",
      "Shape of y: torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed): # Reproducibility를 위한 seed 고정 작업\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Settings\n",
    "\n",
    "use_cuda = torch.cuda.is_available() # if the system supports CUDA -> True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Dataset & augmentation\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize((227, 227)), # image tensor size를 227 x 227로 변환 (input에 넣어주기 위함)\n",
    "            transforms.ToTensor(), # input을 tensor type으로 전환\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # image tensor 값들을 normalization\n",
    "])\n",
    "\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\", # Download=True일 시, 데이터를 다운받을 경로 // Download=False일 시, 데이터가 존재하는 경로\n",
    "    train=True,  # True -> Training set에서 data를 가져옴, False -> Test set에서 data를 가져옴\n",
    "    download=False, # Data가 시스템 내부에 존재하는 지 여부\n",
    "    transform=data_transform, # 위에서 정의한 data transformation을 적용\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False, \n",
    "    download=False,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "batch_size = 128 # 논문에 나와 있는 batch size\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, # dataset from which to load the data \n",
    "    batch_size=batch_size,\n",
    "    pin_memory=use_cuda, # True 시, CUDA memory에 tensor를 올려 놓음\n",
    "    num_workers=multiprocessing.cpu_count()//2, # 원래는 tuning해야 하지만, 일반적인 cpu 개수로 worker 할당\n",
    "    shuffle=True) # True 시 epoch마다 data reshuffle 후 sampling 진행\n",
    "    \n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=use_cuda,\n",
    "    num_workers=multiprocessing.cpu_count()//2,\n",
    "    shuffle=False) # trained model의 정확한 performance 비교를 위해 섞지 않음 \n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): AlexNet(\n",
      "    (net): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (5): ReLU()\n",
      "      (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "      (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU()\n",
      "      (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU()\n",
      "      (14): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -- model\n",
    "\n",
    "model = AlexNet().to(device) # Tensor를 지정한 device에서 연산 진행하도록 하기 위해, to(device)를 붙임 \n",
    "model = torch.nn.DataParallel(model) # 여러 GPU 상에서 병렬 연산 진행\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "        params=model.parameters(), # optimize할 parameter set 지정\n",
    "        lr=0.01, # learning rate 지정\n",
    "        momentum=0.9, # 논문에 나와 있는 SGD momentum 값 \n",
    "        weight_decay=0.0005) # Overfit을 방지하기 위해, weight의 절대적 규모를 전체적으로 감소시키는 역할\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # (step_size) epoch 마다 (gamma) 배씩 learning rate를 감소시켜주는 schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train loss: 2.318789  [    0/50000]\n",
      " Train loss: 2.310215  [ 1600/50000]\n",
      " Train loss: 2.343995  [ 3200/50000]\n",
      " Train loss: 2.369611  [ 4800/50000]\n",
      "Test Error in epoch 0 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.269061  [    0/50000]\n",
      " Train loss: 2.329641  [ 1600/50000]\n",
      " Train loss: 2.375973  [ 3200/50000]\n",
      " Train loss: 2.301568  [ 4800/50000]\n",
      "Test Error in epoch 1 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.377271  [    0/50000]\n",
      " Train loss: 2.340114  [ 1600/50000]\n",
      " Train loss: 2.387733  [ 3200/50000]\n",
      " Train loss: 2.326668  [ 4800/50000]\n",
      "Test Error in epoch 2 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.381385  [    0/50000]\n",
      " Train loss: 2.309510  [ 1600/50000]\n",
      " Train loss: 2.330097  [ 3200/50000]\n",
      " Train loss: 2.367663  [ 4800/50000]\n",
      "Test Error in epoch 3 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.326844  [    0/50000]\n",
      " Train loss: 2.354359  [ 1600/50000]\n",
      " Train loss: 2.377092  [ 3200/50000]\n",
      " Train loss: 2.358562  [ 4800/50000]\n",
      "Test Error in epoch 4 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.343688  [    0/50000]\n",
      " Train loss: 2.363890  [ 1600/50000]\n",
      " Train loss: 2.364630  [ 3200/50000]\n",
      " Train loss: 2.371454  [ 4800/50000]\n",
      "Test Error in epoch 5 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.331225  [    0/50000]\n",
      " Train loss: 2.321694  [ 1600/50000]\n",
      " Train loss: 2.347387  [ 3200/50000]\n",
      " Train loss: 2.373316  [ 4800/50000]\n",
      "Test Error in epoch 6 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.359033  [    0/50000]\n",
      " Train loss: 2.411661  [ 1600/50000]\n",
      " Train loss: 2.364271  [ 3200/50000]\n",
      " Train loss: 2.391378  [ 4800/50000]\n",
      "Test Error in epoch 7 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.330015  [    0/50000]\n",
      " Train loss: 2.341472  [ 1600/50000]\n",
      " Train loss: 2.363574  [ 3200/50000]\n",
      " Train loss: 2.374267  [ 4800/50000]\n",
      "Test Error in epoch 8 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.364206  [    0/50000]\n",
      " Train loss: 2.351048  [ 1600/50000]\n",
      " Train loss: 2.343885  [ 3200/50000]\n",
      " Train loss: 2.339905  [ 4800/50000]\n",
      "Test Error in epoch 9 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.378963  [    0/50000]\n",
      " Train loss: 2.370140  [ 1600/50000]\n",
      " Train loss: 2.358632  [ 3200/50000]\n",
      " Train loss: 2.313260  [ 4800/50000]\n",
      "Test Error in epoch 10 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.397740  [    0/50000]\n",
      " Train loss: 2.355059  [ 1600/50000]\n",
      " Train loss: 2.295773  [ 3200/50000]\n",
      " Train loss: 2.379563  [ 4800/50000]\n",
      "Test Error in epoch 11 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.352314  [    0/50000]\n",
      " Train loss: 2.359674  [ 1600/50000]\n",
      " Train loss: 2.346772  [ 3200/50000]\n",
      " Train loss: 2.385493  [ 4800/50000]\n",
      "Test Error in epoch 12 : \n",
      " Accuracy: 10.0%, Avg loss: 2.333501 \n",
      "\n",
      " Train loss: 2.359985  [    0/50000]\n",
      " Train loss: 2.346533  [ 1600/50000]\n",
      " Train loss: 2.389685  [ 3200/50000]\n",
      " Train loss: 2.359223  [ 4800/50000]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train() # Training과 Inference 시 다르게 작동하는 layer(e.x. Dropout)를 처리해주기 위해, model을 training mode로 전환\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for idx, train_batch in enumerate(train_dataloader):\n",
    "\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device) # model의 input\n",
    "        labels = labels.to(device) # model의 정답 label\n",
    "        \n",
    "        optimizer.zero_grad() # pytorch는 backward 시 gradients 값들을 누적하기 때문에, zero_grad() 를 통해 매 step마다 초기화해주어야 한다.\n",
    "\n",
    "        outs = model(inputs) # model의 predictions\n",
    "        loss = F.cross_entropy(outs, labels) # loss value는 cross entropy로 값을 구함\n",
    "\n",
    "        loss.backward() # 'Require_grad=True'인 모든 tensor에 대한 미분 계산\n",
    "        optimizer.step() # 계산된 loss를 바탕으로, parameter들을 update\n",
    "\n",
    "        if idx % 100 == 0: # 100 iteration 마다 진행\n",
    "            loss, current = loss.item(), idx * len(X) # loss.item() : loss tensor 내 값 반환\n",
    "            size = len(train_dataloader.dataset)      # current / size : 현재 학습 진행 상황\n",
    "            print(f\" Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "    lr_scheduler.step() # stepLR 진행 \n",
    "        \n",
    "    model.eval() # 매 epoch 마다, test set으로 model evaluation을 진행하기 위해서 evaluation mode로 진행\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # no_grad() 는 자동으로 gradient 추적하는 것을 막음 -> 메모리 사용량을 줄이고 연산속도 높이기 위함\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device) # X : input, y : 정답 label\n",
    "            test_pred = model(X)\n",
    "            test_loss += F.cross_entropy(test_pred, y).item()\n",
    "            correct += (torch.argmax(test_pred, dim=-1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error in epoch {epoch+1} : \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소감\n",
    " 먼저, pytorch를 이용해서 모델을 raw하게 구현하는 것은 이번이 처음이어서 코드를 구성하는 데 좀 헤맸던 것 같다. 기존에 내가 pytorch를 공부했을 때 배웠던 것들을 적용해볼 생각이었는데, 아직 실력이 모자라 내 맘대로 구현하기가 쉽지 않았다. 그래서 기본적인 틀을 구현하되 pytorch 기본적인 부분에 대한 설명을 잘 이해하는 것을 목표로 삼았다.\n",
    "\n",
    " 아쉬웠던 점\n",
    " 1. GPU programming\n",
    "    논문에 나와 있는 대로, 특정 층에서 GPU interaction을 통한 연산을 구현해보려고 여러 방법을 찾아봤으나, 쉽지 않았고, 시간을 더 투자해봤자 좋은 결과를 내지 못할 것 같아 포기했다. 따라서 기본 모델 클래스 구현과 Engineering 관점에서 기초적으로 어떤 것들을 적용할 수 있는 지 생각하는 데 초점을 잡았다. 이 부분은 나중에 조금 더 공부해보고 전해보고 싶다.\n",
    " 2. LR schedular 구현\n",
    "    논문에서는 epoch 당 validation set에 대한 accuracy가 plateaus에 다다랐다고 생각되었을 때 learning rate를 0.1씩 감소한다고 나와있다. 그런데 plateaus에 대한 기준을 잡는 게 쉽지 않아, 간단하게 stepLR로 schedular를 짰다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

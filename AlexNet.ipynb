{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AsDQmccqQLgN"},"outputs":[],"source":["import random\n","import multiprocessing\n","import os\n","\n","import numpy as np\n","import torch\n","from torchvision import datasets\n","from torchvision.transforms import transforms\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n"]},{"cell_type":"markdown","metadata":{"id":"fxrcThWHQLgP"},"source":["### Contents\n","1. System settings\n","2. Dataset & Dataloader & Augmentation\n","3. Model\n","4. Optimizer & Loss func\n","5. Training & Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUC_eeTMQLgQ"},"outputs":[],"source":["class AlexNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        \"\"\"\n","        Args:\n","            num_classes (int): 정답 label의 classification 갯수\n","        \"\"\"\n","        \n","        super().__init__() # nn.module을 상속받음\n","        # input size : (b x 3 x 227 x 227)\n","        \n","        self.net = nn.Sequential( # nn.Sequential : model의 층을 연속적으로 쌓아주는 함수\n","            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n","            nn.ReLU(),\n","            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3에 나와 있는 hyperparameter 값 사용  \n","            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n","            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n","            nn.ReLU(),\n","            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n","            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n","            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n","            nn.ReLU(),  \n","            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n","            nn.ReLU(),\n","            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=0.5, inplace=False), # inplace=True : 계산한 값을 변수에 덮어씌움 / True 시 오류가 나서 False로 통일\n","            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5, inplace=False),\n","            nn.Linear(in_features=4096, out_features=4096),\n","            nn.ReLU(),\n","            nn.Linear(in_features=4096, out_features=num_classes),\n","        )\n","        self.init_bias()  # 아래 함수 참고\n","\n","    def init_bias(self):\n","        for layer in self.net: # self.net에 정의된 layer에서\n","            if isinstance(layer, nn.Conv2d): # 해당 layer가 Conv2D layer 라면 \n","                nn.init.normal_(layer.weight, mean=0, std=0.01) # Normal distribution으로 weight initialization 진행\n","                nn.init.constant_(layer.bias, 0) # Bias를 모두 0으로 initialization\n","\n","        nn.init.constant_(self.net[4].bias, 1) # 2번째 Conv2D layer bias를 1로 초기화\n","        nn.init.constant_(self.net[10].bias, 1) # 4번째 Conv2D layer의 bias를 1로 초기화\n","        nn.init.constant_(self.net[12].bias, 1) # 5번째 Conv2D layer의 bias를 1로 초기화\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        x = x.view(-1, 256 * 6 * 6)  # view는 tensor shape를 바꿔줌, FC layer로 넘겨주기 위해 output을 1차원으로 펴주는 역할\n","        return self.classifier(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaaaBRq5QLgR","outputId":"1efb59e7-559a-43ba-d1da-a6d5cc788e97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, H, W]: torch.Size([128, 3, 227, 227])\n","Shape of y: torch.Size([128]) torch.int64\n"]}],"source":["def seed_everything(seed): # Reproducibility를 위한 seed 고정 작업\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","seed_everything(42)\n","\n","# Settings\n","\n","use_cuda = torch.cuda.is_available() # if the system supports CUDA -> True\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","# Dataset & augmentation\n","\n","training_data_transform = transforms.Compose([\n","            transforms.RandomCrop(32, padding=4), # \n","            transforms.ToTensor(), # input을 tensor type으로 전환\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # image tensor 값들을 normalization\n","])\n","\n","test_data_transform = transforms.Compose([\n","            transforms.ToTensor(), # input을 tensor type으로 전환\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # image tensor 값들을 normalization\n","])\n","\n","training_data = datasets.CIFAR10(\n","    root=\"data\", # Download=True일 시, 데이터를 다운받을 경로 // Download=False일 시, 데이터가 존재하는 경로\n","    train=True,  # True -> Training set에서 data를 가져옴, False -> Test set에서 data를 가져옴\n","    download=False, # Data가 시스템 내부에 존재하는 지 여부\n","    transform=data_transform, # 위에서 정의한 data transformation을 적용\n",")\n","\n","test_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False, \n","    download=False,\n","    transform=data_transform,\n",")\n","\n","batch_size = 128 # 논문에 나와 있는 batch size\n","\n","train_dataloader = DataLoader(dataset=training_data, # dataset from which to load the data \n","    batch_size=batch_size,\n","    pin_memory=use_cuda, # True 시, CUDA memory에 tensor를 올려 놓음\n","    num_workers=multiprocessing.cpu_count()//2, # 원래는 tuning해야 하지만, 일반적인 cpu 개수로 worker 할당\n","    shuffle=True) # True 시 epoch마다 data reshuffle 후 sampling 진행\n","    \n","test_dataloader = DataLoader(dataset=test_data,\n","    batch_size=batch_size,\n","    pin_memory=use_cuda,\n","    num_workers=multiprocessing.cpu_count()//2,\n","    shuffle=False) # trained model의 정확한 performance 비교를 위해 섞지 않음 \n","\n","for X, y in test_dataloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DSfEDfxQLgS","outputId":"b7b79f84-75d9-435c-b898-ec830c50afe3"},"outputs":[{"name":"stdout","output_type":"stream","text":["DataParallel(\n","  (module): AlexNet(\n","    (net): Sequential(\n","      (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n","      (1): ReLU()\n","      (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n","      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","      (5): ReLU()\n","      (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n","      (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (9): ReLU()\n","      (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU()\n","      (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU()\n","      (14): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (classifier): Sequential(\n","      (0): Dropout(p=0.5, inplace=False)\n","      (1): Linear(in_features=9216, out_features=4096, bias=True)\n","      (2): ReLU()\n","      (3): Dropout(p=0.5, inplace=False)\n","      (4): Linear(in_features=4096, out_features=4096, bias=True)\n","      (5): ReLU()\n","      (6): Linear(in_features=4096, out_features=10, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["# -- model\n","\n","model = AlexNet().to(device) # Tensor를 지정한 device에서 연산 진행하도록 하기 위해, to(device)를 붙임 \n","model = torch.nn.DataParallel(model) # 여러 GPU 상에서 병렬 연산 진행\n","\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25yJdo3ZQLgS"},"outputs":[],"source":["optimizer = optim.SGD(\n","        params=model.parameters(), # optimize할 parameter set 지정\n","        lr=0.01, # learning rate 지정\n","        momentum=0.9, # 논문에 나와 있는 SGD momentum 값 \n","        weight_decay=0.0005) # Overfit을 방지하기 위해, weight의 절대적 규모를 전체적으로 감소시키는 역할\n","\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # (step_size) epoch 마다 (gamma) 배씩 learning rate를 감소시켜주는 schedular"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJDzQRo3QLgT","outputId":"3e7f15f3-e152-425d-836e-558776f7075e"},"outputs":[],"source":["# Training\n","\n","torch.cuda.empty_cache()\n","\n","for epoch in range(90):\n","    model.train() # Training과 Inference 시 다르게 작동하는 layer(e.x. Dropout)를 처리해주기 위해, model을 training mode로 전환\n","\n","    for idx, train_batch in enumerate(train_dataloader):\n","\n","        inputs, labels = train_batch\n","        inputs = inputs.to(device) # model의 input\n","        labels = labels.to(device) # model의 정답 label\n","        \n","        outs = model(inputs) # model의 predictions\n","        loss = F.cross_entropy(outs, labels) # loss value는 cross entropy로 값을 구함\n","\n","        optimizer.zero_grad() # pytorch는 backward 시 gradients 값들을 누적하기 때문에, zero_grad() 를 통해 매 step마다 초기화해주어야 한다.\n","        loss.backward() # 'Require_grad=True'인 모든 tensor에 대한 미분 계산\n","        optimizer.step() # 계산된 loss를 바탕으로, parameter들을 update\n","\n","        if idx % 100 == 0: # 100 iteration 마다 진행\n","            loss, current = loss.item(), idx * len(X) # loss.item() : loss tensor 내 값 반환\n","            size = len(train_dataloader.dataset)      # current / size : 현재 학습 진행 상황\n","            print(f\" Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","        \n","    lr_scheduler.step() # stepLR 진행 \n","        \n","    model.eval() # 매 epoch 마다, test set으로 model evaluation을 진행하기 위해서 evaluation mode로 진행\n","\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad(): # no_grad() 는 자동으로 gradient 추적하는 것을 막음 -> 메모리 사용량을 줄이고 연산속도 높이기 위함\n","        size = len(test_dataloader.dataset)\n","        num_batches = len(test_dataloader)\n","\n","        for X, y in test_dataloader:\n","            X, y = X.to(device), y.to(device) # X : input, y : 정답 label\n","            test_pred = model(X)\n","            test_loss += F.cross_entropy(test_pred, y).item()\n","            correct += (torch.argmax(test_pred, dim=-1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error in epoch {epoch+1} : \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBL8decyQLgT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZN-VqLjBQLgU"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huQrev9tQLgU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bMnlTiyQLgV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zU94FQa4QLgV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPtJV0lwQLgW"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.7.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.14"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
